{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41a6aa6e",
   "metadata": {},
   "source": [
    "1. Normaliza√ß√£o de texto e Remo√ß√£o de Ru√≠do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe1f788",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "original = \"Ol√°!!! Este √© um exemplo de texto, com v√°rias PONTUA√á√ïES, s√≠mbolos #especiais, e LETRAS mai√∫sculas.\"\n",
    "\n",
    "texto_limpo = re.sub(r'[^A-Za-z√Ä-√ø\\s]', '',original)\n",
    "\n",
    "texto_normalizado = texto_limpo.lower()\n",
    "\n",
    "print(f'Texto original: {original}')\n",
    "print(f'\\nTexto limpo: {texto_limpo}')\n",
    "print(f'\\nTexto normalizado: {texto_normalizado}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9466578",
   "metadata": {},
   "source": [
    "2. Tokeniza√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86952dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8f3c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "tokens = word_tokenize(texto_normalizado)\n",
    "\n",
    "print(f'Texto original: {original}')\n",
    "print(f'\\n\\nTexto limpo: {texto_limpo}')\n",
    "print(f'\\n\\nTexto normalizado: {texto_normalizado}')\n",
    "print(f'\\n\\nTokens extraidos: {tokens}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d16c52c",
   "metadata": {},
   "source": [
    "3. Remo√ß√£o de Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad74c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stopwords_pt = set(stopwords.words('portuguese'))\n",
    "\n",
    "print(stopwords_pt)\n",
    "\n",
    "tokens_sem_stopwords = [palavra for palavra in tokens if palavra.lower() not in stopwords_pt]\n",
    "\n",
    "print(f'\\n\\nTokens extraidos: {tokens} + \\n quantidade de tokens: {len(tokens)}')\n",
    "print(f'\\n\\nTokens extraidos: {tokens_sem_stopwords} + \\n quantidade de tokens: {len(tokens_sem_stopwords)}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831f737f",
   "metadata": {},
   "source": [
    "4. Stemming e Lemaliza√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603ef7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.stem import RSLPStemmer\n",
    "\n",
    "nltk.download('rslp')\n",
    "\n",
    "stemmer = RSLPStemmer()\n",
    "stemming = [stemmer.stem(palavra) for palavra in tokens_sem_stopwords]\n",
    "print(f'\\n\\nTokens extraidos: {tokens_sem_stopwords}')\n",
    "print(f'\\n\\nTokens radicais: {stemming}\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2692c1c3",
   "metadata": {},
   "source": [
    "5. Exemplo 01 - Pr√© Processamento completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc735c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "texto = input(\"Insira um texto que seja coerente, podendo ter s√≠mbolos: \")\n",
    "\n",
    "texto_limpo = re.sub(r'[^a-zA-Z]', ' ', texto)\n",
    "texto_lower = texto_limpo.lower()\n",
    "\n",
    "tokens = nltk.word_tokenize(texto_lower)\n",
    "\n",
    "stop_words = set(stopwords.words('portuguese'))\n",
    "palavras_filtradas = [palavra for palavra in tokens if palavra not in stop_words]\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "palavras_stemizadas = [stemmer.stem(palavra) for palavra in palavras_filtradas]\n",
    "\n",
    "print(palavras_stemizadas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14a5c9c",
   "metadata": {},
   "source": [
    "Exemplo 02 - Estrutura de Pr√©-processamento de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df1c879",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download pt_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b023ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('rslp')\n",
    "\n",
    "# (Trocar para 'en_core_web_sm' se for ingl√™s)\n",
    "nlp = spacy.load(\"pt_core_news_sm\")\n",
    "\n",
    "texto = \"O Processamento de Linguagem Natural (PLN) √© uma √°rea essencial da intelig√™ncia artificial! üòä Confira mais em: https://exemplo.com\"\n",
    "\n",
    "def normalizar_texto(texto):\n",
    "    texto = texto.lower()\n",
    "    texto = re.sub(r'https?://\\S+|www\\.\\S+', '', texto)\n",
    "    texto = re.sub(r'[^a-zA-Z√°-√∫√Å-√ö√ß√á ]', '', texto)\n",
    "    return texto\n",
    "\n",
    "texto_normalizado = normalizar_texto(texto)\n",
    "\n",
    "tokens = nltk.word_tokenize(texto_normalizado, language='portuguese')\n",
    "\n",
    "stopwords_pt = set(stopwords.words('portuguese'))\n",
    "tokens_sem_stopwords = [token for token in tokens if token not in stopwords_pt]\n",
    "\n",
    "stemmer = nltk.RSLPStemmer()\n",
    "tokens_stem = [stemmer.stem(token) for token in tokens_sem_stopwords]\n",
    "\n",
    "def lematizar_com_spacy(tokens):\n",
    "    doc = nlp(\" \".join(tokens))\n",
    "    return [token.lemma_ for token in doc]\n",
    "\n",
    "tokens_lematizados = lematizar_com_spacy(tokens_sem_stopwords)\n",
    "\n",
    "print(\"Texto Original:\\n\", texto)\n",
    "print(\"\\nTexto Normalizado:\\n\", texto_normalizado)\n",
    "print(\"\\nTokens:\\n\", tokens)\n",
    "print(\"\\nTokens Sem Stopwords:\\n\", tokens_sem_stopwords)\n",
    "print(\"\\nStemming:\\n\", tokens_stem)\n",
    "print(\"\\nLematiza√ß√£o:\\n\", tokens_lematizados)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b624c870",
   "metadata": {},
   "source": [
    "Exemplo 03 - O modelo pre treinado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861a809c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"pt_core_news_sm\")\n",
    "\n",
    "textoRecebido = input(\"Digite um texto para ser analisado: \")\n",
    "doc = nlp(textoRecebido)\n",
    "\n",
    "print('\\nAn√°lise gramatical das palavras:')\n",
    "for token in doc:\n",
    "    print(f\"Palavra: {token.text}, Classe: {token.pos_}\")\n",
    "\n",
    "print(\"\\nAnalise de Depend√™ncias:\")\n",
    "for token in doc:\n",
    "  print(f\"Palavra: {token.text}, Depende de: {token.head.text}\")\n",
    "\n",
    "from spacy import displacy\n",
    "displacy.render(doc, style=\"dep\", jupyter=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
